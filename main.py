"""
AstrBot ç½‘é¡µåˆ†ææ’ä»¶

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç½‘é¡µåˆ†ææ’ä»¶ï¼Œèƒ½å¤Ÿï¼š
1. è‡ªåŠ¨è¯†åˆ«ç”¨æˆ·å‘é€çš„ç½‘é¡µé“¾æ¥
2. æ™ºèƒ½æŠ“å–ç½‘é¡µå†…å®¹
3. è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œæ·±åº¦åˆ†æå’Œæ€»ç»“
4. æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼å’Œè‡ªå®šä¹‰é…ç½®
5. æä¾›ä¸°å¯Œçš„å‘½ä»¤å’Œç®¡ç†åŠŸèƒ½

ä½¿ç”¨æ–¹å¼ï¼š
- è‡ªåŠ¨æ¨¡å¼ï¼šç›´æ¥å‘é€åŒ…å«ç½‘é¡µé“¾æ¥çš„æ¶ˆæ¯å³å¯
- æ‰‹åŠ¨æ¨¡å¼ï¼šä½¿ç”¨ /ç½‘é¡µåˆ†æ å‘½ä»¤ï¼Œå¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com
- æ”¯æŒå¤šç§æŒ‡ä»¤åˆ«åï¼šåˆ†æã€æ€»ç»“ã€webã€analyze

æ’ä»¶ç‰¹ç‚¹ï¼š
- æ”¯æŒå¹¶å‘å¤„ç†å¤šä¸ªURL
- å†…ç½®ç¼“å­˜æœºåˆ¶ï¼Œæé«˜å“åº”é€Ÿåº¦
- æ”¯æŒåŸŸåç™½åå•å’Œé»‘åå•
- æä¾›æˆªå›¾åŠŸèƒ½
- æ”¯æŒå†…å®¹ç¿»è¯‘
- æ”¯æŒå¤šç§åˆ†æç»“æœå¯¼å‡ºæ ¼å¼
"""

from typing import List

from astrbot.api import AstrBotConfig
from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger

from .analyzer import WebAnalyzer
from .cache import CacheManager


@register("astrbot_plugin_web_analyzer", "Sakura520222", "è‡ªåŠ¨è¯†åˆ«ç½‘é¡µé“¾æ¥å¹¶è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“", "1.2.3", "https://github.com/Sakura520222/astrbot_plugin_web_analyzer")
class WebAnalyzerPlugin(Star):
    """ç½‘é¡µåˆ†ææ’ä»¶ä¸»ç±»
    
    è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒç±»ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼ŒåŒ…æ‹¬ï¼š
    - é…ç½®ç®¡ç†å’ŒéªŒè¯
    - æ¶ˆæ¯äº‹ä»¶å¤„ç†
    - URLæå–å’ŒéªŒè¯
    - ç½‘é¡µå†…å®¹æŠ“å–å’Œåˆ†æ
    - LLMè°ƒç”¨å’Œç»“æœç”Ÿæˆ
    - ç¼“å­˜ç®¡ç†
    - å„ç§å‘½ä»¤å¤„ç†
    
    æ’ä»¶æ”¯æŒè‡ªåŠ¨å’Œæ‰‹åŠ¨ä¸¤ç§åˆ†ææ¨¡å¼ï¼Œ
    å¹¶æä¾›äº†ä¸°å¯Œçš„é…ç½®é€‰é¡¹å’Œç®¡ç†å‘½ä»¤ã€‚
    """
    
    def __init__(self, context: Context, config: AstrBotConfig):
        """æ’ä»¶åˆå§‹åŒ–æ–¹æ³•
        
        è´Ÿè´£åŠ è½½å’ŒéªŒè¯é…ç½®ï¼Œåˆå§‹åŒ–å„ç§åŠŸèƒ½ç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
        - åŸºæœ¬é…ç½®ï¼ˆè¶…æ—¶ã€é‡è¯•ã€ç”¨æˆ·ä»£ç†ç­‰ï¼‰
        - åŸŸåæ§åˆ¶ï¼ˆå…è®¸/ç¦æ­¢åŸŸåï¼‰
        - åˆ†æè®¾ç½®ï¼ˆemojiã€ç»Ÿè®¡ä¿¡æ¯ã€æ‘˜è¦é•¿åº¦ç­‰ï¼‰
        - æˆªå›¾è®¾ç½®ï¼ˆè´¨é‡ã€å°ºå¯¸ã€æ ¼å¼ç­‰ï¼‰
        - LLMé…ç½®ï¼ˆæä¾›å•†ã€è‡ªå®šä¹‰æç¤ºè¯ç­‰ï¼‰
        - ç¾¤èŠé»‘åå•
        - ç¿»è¯‘è®¾ç½®
        - ç¼“å­˜é…ç½®
        - å†…å®¹æå–è®¾ç½®
        
        æ‰€æœ‰é…ç½®é¡¹éƒ½ä¼šè¿›è¡Œåˆç†æ€§éªŒè¯ï¼Œå¹¶è®¾ç½®é»˜è®¤å€¼ï¼Œç¡®ä¿æ’ä»¶ç¨³å®šè¿è¡Œã€‚
        """
        super().__init__(context)
        self.config = config
        
        # åŸºæœ¬é…ç½®åŠ è½½ä¸éªŒè¯
        # æœ€å¤§å†…å®¹é•¿åº¦ï¼šé™åˆ¶æŠ“å–çš„ç½‘é¡µå†…å®¹å¤§å°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_content_length = max(1000, config.get('max_content_length', 10000))
        # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼šè®¾ç½®åˆç†çš„è¶…æ—¶èŒƒå›´ï¼Œé¿å…è¯·æ±‚è¿‡é•¿æ—¶é—´é˜»å¡
        self.timeout = max(5, min(300, config.get('request_timeout', 30)))
        # é‡è¯•æ¬¡æ•°ï¼šè¯·æ±‚å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°
        self.retry_count = max(0, min(10, config.get('retry_count', 3)))
        # é‡è¯•å»¶è¿Ÿï¼šæ¯æ¬¡é‡è¯•ä¹‹é—´çš„ç­‰å¾…æ—¶é—´
        self.retry_delay = max(0, min(10, config.get('retry_delay', 2)))
        # æ˜¯å¦å¯ç”¨LLMåˆ†æï¼šæ§åˆ¶æ˜¯å¦è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ
        self.llm_enabled = bool(config.get('llm_enabled', True))
        # æ˜¯å¦è‡ªåŠ¨åˆ†æï¼šæ§åˆ¶æ˜¯å¦è‡ªåŠ¨è¯†åˆ«æ¶ˆæ¯ä¸­çš„é“¾æ¥å¹¶åˆ†æ
        self.auto_analyze = bool(config.get('auto_analyze', True))
        # ç”¨æˆ·ä»£ç†ï¼šç”¨äºæ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚ï¼Œé¿å…è¢«ç½‘ç«™å°ç¦
        self.user_agent = config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        # ä»£ç†è®¾ç½®ï¼šç”¨äºç½‘ç»œä»£ç†ï¼ŒåŠ é€Ÿæˆ–ç»•è¿‡ç½‘ç»œé™åˆ¶
        self.proxy = config.get('proxy', '')
        
        # éªŒè¯ä»£ç†æ ¼å¼æ˜¯å¦æ­£ç¡®
        if self.proxy:
            try:
                from urllib.parse import urlparse
                parsed = urlparse(self.proxy)
                if not all([parsed.scheme, parsed.netloc]):
                    logger.warning(f"æ— æ•ˆçš„ä»£ç†æ ¼å¼: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®")
                    self.proxy = ''
            except Exception as e:
                logger.warning(f"è§£æä»£ç†å¤±è´¥: {self.proxy}ï¼Œå°†å¿½ç•¥ä»£ç†è®¾ç½®ï¼Œé”™è¯¯: {e}")
                self.proxy = ''
        
        # è§£æå…è®¸å’Œç¦æ­¢çš„åŸŸååˆ—è¡¨
        self.allowed_domains = self._parse_domain_list(config.get('allowed_domains', ''))
        self.blocked_domains = self._parse_domain_list(config.get('blocked_domains', ''))
        
        # åˆ†æè®¾ç½®éªŒè¯
        analysis_settings = config.get('analysis_settings', {})
        # æ˜¯å¦åœ¨ç»“æœä¸­ä½¿ç”¨emoji
        self.enable_emoji = bool(analysis_settings.get('enable_emoji', True))
        # æ˜¯å¦æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡ä¿¡æ¯
        self.enable_statistics = bool(analysis_settings.get('enable_statistics', True))
        # æœ€å¤§æ‘˜è¦é•¿åº¦ï¼šé™åˆ¶LLMç”Ÿæˆçš„æ‘˜è¦å¤§å°
        self.max_summary_length = max(500, min(10000, analysis_settings.get('max_summary_length', 2000)))
        
        # æˆªå›¾è®¾ç½®éªŒè¯
        self.enable_screenshot = bool(analysis_settings.get('enable_screenshot', True))
        # æˆªå›¾è´¨é‡ï¼šæ§åˆ¶æˆªå›¾çš„æ¸…æ™°åº¦å’Œæ–‡ä»¶å¤§å°
        self.screenshot_quality = max(10, min(100, analysis_settings.get('screenshot_quality', 80)))
        # æˆªå›¾å®½åº¦å’Œé«˜åº¦ï¼šæ§åˆ¶æˆªå›¾çš„åˆ†è¾¨ç‡
        self.screenshot_width = max(320, min(4096, analysis_settings.get('screenshot_width', 1280)))
        self.screenshot_height = max(240, min(4096, analysis_settings.get('screenshot_height', 720)))
        # æ˜¯å¦æˆªå–æ•´é¡µï¼šæ§åˆ¶æ˜¯å¦æˆªå–å®Œæ•´çš„ç½‘é¡µå†…å®¹
        self.screenshot_full_page = bool(analysis_settings.get('screenshot_full_page', False))
        # æˆªå›¾ç­‰å¾…æ—¶é—´ï¼šé¡µé¢åŠ è½½å®Œæˆåç­‰å¾…çš„æ—¶é—´ï¼Œç¡®ä¿å†…å®¹å®Œæ•´æ˜¾ç¤º
        self.screenshot_wait_time = max(0, min(10000, analysis_settings.get('screenshot_wait_time', 2000)))
        
        # éªŒè¯æˆªå›¾æ ¼å¼æ˜¯å¦æ”¯æŒ
        screenshot_format = analysis_settings.get('screenshot_format', 'jpeg').lower()
        if screenshot_format not in ['jpeg', 'png']:
            logger.warning(f"æ— æ•ˆçš„æˆªå›¾æ ¼å¼: {screenshot_format}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼ jpeg")
            self.screenshot_format = 'jpeg'
        else:
            self.screenshot_format = screenshot_format
        
        # LLMæä¾›å•†é…ç½®ï¼šæŒ‡å®šä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
        self.llm_provider = config.get('llm_provider', '')
        
        # ç¾¤èŠé»‘åå•é…ç½®ï¼šç”¨äºæ§åˆ¶å“ªäº›ç¾¤èŠä¸å…è®¸ä½¿ç”¨æ’ä»¶
        group_blacklist_text = config.get('group_blacklist', '')
        self.group_blacklist = self._parse_group_list(group_blacklist_text)
        
        # åˆå¹¶è½¬å‘é…ç½®ï¼šæ§åˆ¶æ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘åŠŸèƒ½å‘é€åˆ†æç»“æœ
        self.merge_forward_enabled = bool(config.get('merge_forward_enabled', False))
        
        # è‡ªå®šä¹‰æç¤ºè¯é…ç½®ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰LLMåˆ†æçš„æç¤ºè¯
        self.custom_prompt = config.get('custom_prompt', '')
        
        # ç¿»è¯‘è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦è‡ªåŠ¨ç¿»è¯‘ç½‘é¡µå†…å®¹
        translation_settings = config.get('translation_settings', {})
        self.enable_translation = bool(translation_settings.get('enable_translation', False))
        
        # éªŒè¯ç›®æ ‡è¯­è¨€æ˜¯å¦æ”¯æŒ
        self.target_language = translation_settings.get('target_language', 'zh').lower()
        valid_languages = ['zh', 'en', 'ja', 'ko', 'fr', 'de', 'es', 'ru', 'ar', 'pt']
        if self.target_language not in valid_languages:
            logger.warning(f"æ— æ•ˆçš„ç›®æ ‡è¯­è¨€: {self.target_language}ï¼Œå°†ä½¿ç”¨é»˜è®¤è¯­è¨€ zh")
            self.target_language = 'zh'
        
        # ç¿»è¯‘æä¾›å•†é…ç½®
        self.translation_provider = translation_settings.get('translation_provider', 'llm')
        # è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯ï¼šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰ç¿»è¯‘çš„æç¤ºè¯
        self.custom_translation_prompt = translation_settings.get('custom_translation_prompt', '')
        
        # ç¼“å­˜è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦å¯ç”¨ç»“æœç¼“å­˜
        cache_settings = config.get('cache_settings', {})
        self.enable_cache = bool(cache_settings.get('enable_cache', True))
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼šæ§åˆ¶ç¼“å­˜ç»“æœçš„æœ‰æ•ˆæœŸ
        self.cache_expire_time = max(5, min(10080, cache_settings.get('cache_expire_time', 1440)))
        # æœ€å¤§ç¼“å­˜æ•°é‡ï¼šæ§åˆ¶ç¼“å­˜çš„æœ€å¤§æ¡ç›®æ•°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡é«˜
        self.max_cache_size = max(10, min(1000, cache_settings.get('max_cache_size', 100)))
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼šç”¨äºç®¡ç†åˆ†æç»“æœçš„ç¼“å­˜
        self.cache_manager = CacheManager(
            max_size=self.max_cache_size,
            expire_time=self.cache_expire_time
        )
        
        # å†…å®¹æå–è®¾ç½®éªŒè¯ï¼šæ§åˆ¶æ˜¯å¦å¯ç”¨ç‰¹å®šå†…å®¹æå–
        content_extraction_settings = config.get('content_extraction_settings', {})
        self.enable_specific_extraction = bool(content_extraction_settings.get('enable_specific_extraction', False))
        # æå–ç±»å‹ï¼šæŒ‡å®šè¦æå–çš„å†…å®¹ç±»å‹
        extract_types_text = content_extraction_settings.get('extract_types', 'title\ncontent')
        self.extract_types = [t.strip() for t in extract_types_text.split('\n') if t.strip()]
        
        # éªŒè¯æå–ç±»å‹æ˜¯å¦æœ‰æ•ˆ
        valid_extract_types = ['title', 'content', 'images', 'links', 'tables', 'lists', 'code', 'meta']
        invalid_types = [t for t in self.extract_types if t not in valid_extract_types]
        if invalid_types:
            logger.warning(f"æ— æ•ˆçš„æå–ç±»å‹: {', '.join(invalid_types)}ï¼Œå°†å¿½ç•¥è¿™äº›ç±»å‹")
            self.extract_types = [t for t in self.extract_types if t in valid_extract_types]
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæå–ç±»å‹
        if not self.extract_types:
            self.extract_types = ['title', 'content']
        
        # è‡ªåŠ¨æ·»åŠ metaç±»å‹ï¼Œç”¨äºæå–ç½‘é¡µå…ƒä¿¡æ¯
        if 'meta' not in self.extract_types:
            self.extract_types.append('meta')
        
        # åˆå§‹åŒ–ç½‘é¡µåˆ†æå™¨ï¼šç”¨äºæŠ“å–å’Œåˆ†æç½‘é¡µå†…å®¹
        self.analyzer = WebAnalyzer(
            max_content_length=self.max_content_length, 
            timeout=self.timeout, 
            user_agent=self.user_agent, 
            proxy=self.proxy, 
            retry_count=self.retry_count, 
            retry_delay=self.retry_delay
        )
        
        # URLå¤„ç†æ ‡å¿—é›†åˆï¼šç”¨äºé¿å…é‡å¤å¤„ç†åŒä¸€URL
        self.processing_urls = set()
        
        # è®°å½•é…ç½®åˆå§‹åŒ–å®Œæˆ
        logger.info("æ’ä»¶é…ç½®åˆå§‹åŒ–å®Œæˆ")
    
    def _parse_domain_list(self, domain_text: str) -> List[str]:
        """å°†åŸŸååˆ—è¡¨æ–‡æœ¬è§£æä¸ºåˆ—è¡¨æ ¼å¼
        
        è¯¥æ–¹æ³•ç”¨äºå¤„ç†é…ç½®ä¸­çš„åŸŸååˆ—è¡¨ï¼Œå°†å¤šè¡Œæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨
        
        Args:
            domain_text: åŒ…å«åŸŸåçš„æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªåŸŸå
            
        Returns:
            è§£æåçš„åŸŸååˆ—è¡¨ï¼Œå·²å»é™¤ç©ºè¡Œå’Œç©ºç™½å­—ç¬¦
        """
        if not domain_text:
            return []
        domains = [domain.strip() for domain in domain_text.split('\n') if domain.strip()]
        return domains
    
    def _parse_group_list(self, group_text: str) -> List[str]:
        """å°†ç¾¤èŠåˆ—è¡¨æ–‡æœ¬è§£æä¸ºåˆ—è¡¨æ ¼å¼
        
        è¯¥æ–¹æ³•ç”¨äºå¤„ç†é…ç½®ä¸­çš„ç¾¤èŠé»‘åå•ï¼Œå°†å¤šè¡Œæ–‡æœ¬è½¬æ¢ä¸ºPythonåˆ—è¡¨
        
        Args:
            group_text: åŒ…å«ç¾¤èŠIDçš„æ–‡æœ¬ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            
        Returns:
            è§£æåçš„ç¾¤èŠIDåˆ—è¡¨ï¼Œå·²å»é™¤ç©ºè¡Œå’Œç©ºç™½å­—ç¬¦
        """
        if not group_text:
            return []
        groups = [group.strip() for group in group_text.split('\n') if group.strip()]
        return groups
    
    def _is_group_blacklisted(self, group_id: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­
        
        Args:
            group_id: ç¾¤èŠID
            
        Returns:
            Trueè¡¨ç¤ºåœ¨é»‘åå•ä¸­ï¼ŒFalseè¡¨ç¤ºä¸åœ¨é»‘åå•ä¸­
        """
        if not group_id or not self.group_blacklist:
            return False
        return group_id in self.group_blacklist
    
    def _is_domain_allowed(self, url: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šURLçš„åŸŸåæ˜¯å¦å…è®¸è®¿é—®
        
        è¯¥æ–¹æ³•æ ¹æ®é…ç½®çš„å…è®¸å’Œç¦æ­¢åŸŸååˆ—è¡¨ï¼Œåˆ¤æ–­URLæ˜¯å¦å¯ä»¥è®¿é—®
        
        è®¿é—®è§„åˆ™ï¼š
        1. å¦‚æœåŸŸååœ¨ç¦æ­¢åˆ—è¡¨ä¸­ï¼Œç›´æ¥æ‹’ç»
        2. å¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼Œåªæœ‰åœ¨åˆ—è¡¨ä¸­çš„åŸŸåæ‰å…è®¸è®¿é—®
        3. å¦‚æœå…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å…è®¸æ‰€æœ‰æœªè¢«ç¦æ­¢çš„åŸŸå
        
        Args:
            url: è¦æ£€æŸ¥çš„URL
            
        Returns:
            Trueè¡¨ç¤ºå…è®¸è®¿é—®ï¼ŒFalseè¡¨ç¤ºç¦æ­¢è®¿é—®
        """
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢åˆ—è¡¨ä¸­
            if self.blocked_domains:
                for blocked_domain in self.blocked_domains:
                    if blocked_domain.lower() in domain:
                        return False
            
            # ç„¶åæ£€æŸ¥æ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼ˆå¦‚æœå…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼‰
            if self.allowed_domains:
                for allowed_domain in self.allowed_domains:
                    if allowed_domain.lower() in domain:
                        return True
                return False  # å…è®¸åˆ—è¡¨ä¸ä¸ºç©ºï¼Œä½†åŸŸåä¸åœ¨å…¶ä¸­ï¼Œæ‹’ç»è®¿é—®
            
            return True  # å…è®¸åˆ—è¡¨ä¸ºç©ºï¼Œå…è®¸æ‰€æœ‰æœªè¢«ç¦æ­¢çš„åŸŸå
        except Exception:
            return False
    
    @filter.command("ç½‘é¡µåˆ†æ", alias={'åˆ†æ', 'æ€»ç»“', 'web', 'analyze'})
    async def analyze_webpage(self, event: AstrMessageEvent):
        """æ‰‹åŠ¨åˆ†ææŒ‡å®šç½‘é¡µé“¾æ¥
        
        è¿™æ˜¯æ’ä»¶çš„æ ¸å¿ƒå‘½ä»¤ï¼Œç”¨äºæ‰‹åŠ¨è§¦å‘ç½‘é¡µåˆ†æåŠŸèƒ½
        
        ç”¨æ³•ç¤ºä¾‹ï¼š
        /ç½‘é¡µåˆ†æ https://example.com
        /åˆ†æ https://example.com
        /æ€»ç»“ https://example.com
        
        æ”¯æŒåŒæ—¶åˆ†æå¤šä¸ªé“¾æ¥ï¼Œæ’ä»¶ä¼šè‡ªåŠ¨å¤„ç†å¹¶è¿”å›ç»“æœ
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        message_text = event.message_str
        
        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            yield event.plain_result("è¯·æä¾›è¦åˆ†æçš„ç½‘é¡µé“¾æ¥ï¼Œä¾‹å¦‚ï¼š/ç½‘é¡µåˆ†æ https://example.com")
            return
        
        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥ï¼Œè¯·æ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return
        
        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            yield event.plain_result("æ‰€æœ‰åŸŸåéƒ½ä¸åœ¨å…è®¸è®¿é—®çš„åˆ—è¡¨ä¸­ï¼Œæˆ–å·²è¢«ç¦æ­¢è®¿é—®")
            return
        
        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ­£åœ¨åˆ†æç½‘é¡µ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ­£åœ¨åˆ†æ{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    @filter.event_message_type(filter.EventMessageType.ALL)
    async def auto_detect_urls(self, event: AstrMessageEvent):
        """è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„URLé“¾æ¥å¹¶è¿›è¡Œåˆ†æ
        
        è¯¥æ–¹æ³•ä¼šè‡ªåŠ¨ç›‘å¬æ‰€æœ‰æ¶ˆæ¯ï¼Œå½“æ£€æµ‹åˆ°åŒ…å«URLçš„æ¶ˆæ¯æ—¶ï¼Œ
        ä¼šè‡ªåŠ¨è§¦å‘ç½‘é¡µåˆ†æåŠŸèƒ½ï¼Œæ— éœ€ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨å‘½ä»¤
        
        è‡ªåŠ¨åˆ†æè§„åˆ™ï¼š
        1. ä»…å½“auto_analyzeé…ç½®é¡¹ä¸ºTrueæ—¶å¯ç”¨
        2. è·³è¿‡æ‰€æœ‰ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        3. è·³è¿‡åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤å…³é”®å­—çš„æ¶ˆæ¯
        4. è·³è¿‡åœ¨é»‘åå•ä¸­çš„ç¾¤èŠæ¶ˆæ¯
        5. ä»…å¤„ç†æ ¼å¼æ­£ç¡®ä¸”åœ¨å…è®¸åŸŸååˆ—è¡¨ä¸­çš„URL
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨åˆ†æåŠŸèƒ½
        if not self.auto_analyze:
            logger.info("è‡ªåŠ¨åˆ†æåŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæŒ‡ä»¤è°ƒç”¨ï¼Œé¿å…é‡å¤å¤„ç†
        message_text = event.message_str.strip()
        
        # æ–¹æ³•1ï¼šè·³è¿‡ä»¥/å¼€å¤´çš„æŒ‡ä»¤æ¶ˆæ¯
        if message_text.startswith('/'):
            logger.info("æ£€æµ‹åˆ°æŒ‡ä»¤è°ƒç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return
        
        # æ–¹æ³•2ï¼šæ£€æŸ¥äº‹ä»¶æ˜¯å¦æœ‰commandå±æ€§ï¼ˆæŒ‡ä»¤è°ƒç”¨æ—¶ä¼šæœ‰ï¼‰
        if hasattr(event, 'command'):
            logger.info("æ£€æµ‹åˆ°commandå±æ€§ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
            return
        
        # æ–¹æ³•3ï¼šæ£€æŸ¥åŸå§‹æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤å…³é”®å­—
        raw_message = None
        if hasattr(event, 'raw_message'):
            raw_message = str(event.raw_message)
        elif hasattr(event, 'message_obj'):
            raw_message = str(event.message_obj)
        
        if raw_message:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç½‘é¡µåˆ†æç›¸å…³æŒ‡ä»¤
            command_keywords = ['ç½‘é¡µåˆ†æ', '/åˆ†æ', '/æ€»ç»“', '/web', '/analyze']
            for keyword in command_keywords:
                if keyword in raw_message:
                    logger.info(f"æ£€æµ‹åˆ°æŒ‡ä»¤å…³é”®å­— {keyword}ï¼Œè·³è¿‡è‡ªåŠ¨åˆ†æ")
                    return
        
        # æ£€æŸ¥ç¾¤èŠæ˜¯å¦åœ¨é»‘åå•ä¸­ï¼ˆä»…ç¾¤èŠæ¶ˆæ¯ï¼‰
        group_id = None
        
        # æ–¹æ³•1ï¼šä»äº‹ä»¶å¯¹è±¡ç›´æ¥è·å–ç¾¤èŠID
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        # æ–¹æ³•2ï¼šä»æ¶ˆæ¯å¯¹è±¡è·å–ç¾¤èŠID
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        # æ–¹æ³•3ï¼šä»åŸå§‹æ¶ˆæ¯è·å–ç¾¤èŠID
        elif hasattr(event, 'raw_message') and hasattr(event.raw_message, 'group_id') and event.raw_message.group_id:
            group_id = event.raw_message.group_id
        
        # ç¾¤èŠåœ¨é»‘åå•ä¸­æ—¶é™é»˜å¿½ç•¥ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        if group_id and self._is_group_blacklisted(group_id):
            return
            
        # ä»æ¶ˆæ¯ä¸­æå–æ‰€æœ‰URL
        urls = self.analyzer.extract_urls(message_text)
        if not urls:
            return  # æ²¡æœ‰URLï¼Œä¸å¤„ç†
        
        # éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®
        valid_urls = [url for url in urls if self.analyzer.is_valid_url(url)]
        if not valid_urls:
            return  # æ²¡æœ‰æœ‰æ•ˆURLï¼Œä¸å¤„ç†
        
        # è¿‡æ»¤æ‰ä¸å…è®¸è®¿é—®çš„åŸŸå
        allowed_urls = [url for url in valid_urls if self._is_domain_allowed(url)]
        if not allowed_urls:
            return  # æ²¡æœ‰å…è®¸è®¿é—®çš„URLï¼Œä¸å¤„ç†
        
        # å‘é€å¤„ç†æç¤ºæ¶ˆæ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†æ
        if len(allowed_urls) == 1:
            yield event.plain_result(f"æ£€æµ‹åˆ°ç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ: {allowed_urls[0]}")
        else:
            yield event.plain_result(f"æ£€æµ‹åˆ°{len(allowed_urls)}ä¸ªç½‘é¡µé“¾æ¥ï¼Œæ­£åœ¨åˆ†æ...")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰å…è®¸è®¿é—®çš„URL
        async for result in self._batch_process_urls(event, allowed_urls):
            yield result
    
    async def _process_single_url(self, event: AstrMessageEvent, url: str, analyzer: WebAnalyzer) -> dict:
        """å¤„ç†å•ä¸ªURLï¼Œè¿”å›å®Œæ•´çš„åˆ†æç»“æœ
        
        è¿™æ˜¯å¤„ç†å•ä¸ªç½‘é¡µé“¾æ¥çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£ï¼š
        1. æ£€æŸ¥ç¼“å­˜
        2. æŠ“å–ç½‘é¡µå†…å®¹
        3. æå–ç½‘é¡µå†…å®¹
        4. ç¿»è¯‘å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        5. è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        6. æå–ç‰¹å®šç±»å‹å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        7. æ•è·ç½‘é¡µæˆªå›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        8. æ›´æ–°ç¼“å­˜
        9. è¿”å›å®Œæ•´çš„åˆ†æç»“æœ
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            url: è¦åˆ†æçš„ç½‘é¡µURL
            analyzer: WebAnalyzerå®ä¾‹ï¼Œç”¨äºæŠ“å–å’Œåˆ†æç½‘é¡µ
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
            {
                'url': åˆ†æçš„URL,
                'result': åˆ†æç»“æœæ–‡æœ¬,
                'screenshot': ç½‘é¡µæˆªå›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            }
        """
        try:
            # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
            cached_result = self._check_cache(url)
            if cached_result:
                logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {url}")
                return cached_result
            
            # æŠ“å–ç½‘é¡µHTMLå†…å®¹
            html = await analyzer.fetch_webpage(url)
            if not html:
                return {
                    'url': url,
                    'result': f"âŒ æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}",
                    'screenshot': None
                }
            
            # ä»HTMLä¸­æå–ç»“æ„åŒ–å†…å®¹
            content_data = analyzer.extract_content(html, url)
            if not content_data:
                return {
                    'url': url,
                    'result': f"âŒ æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}",
                    'screenshot': None
                }
            
            # å¦‚æœå¯ç”¨äº†ç¿»è¯‘åŠŸèƒ½ï¼Œå…ˆç¿»è¯‘å†…å®¹
            if self.enable_translation:
                translated_content = await self._translate_content(event, content_data['content'])
                # åˆ›å»ºç¿»è¯‘åçš„å†…å®¹æ•°æ®å‰¯æœ¬
                translated_content_data = content_data.copy()
                translated_content_data['content'] = translated_content
                # è°ƒç”¨LLMè¿›è¡Œåˆ†æï¼ˆä½¿ç”¨ç¿»è¯‘åçš„å†…å®¹ï¼‰
                analysis_result = await self.analyze_with_llm(event, translated_content_data)
            else:
                # ç›´æ¥è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                analysis_result = await self.analyze_with_llm(event, content_data)
            
            # å¦‚æœå¯ç”¨äº†ç‰¹å®šå†…å®¹æå–ï¼Œæå–é¢å¤–ä¿¡æ¯
            specific_content = self._extract_specific_content(html, url)
            if specific_content:
                # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"
                
                # æ·»åŠ å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'images' in specific_content and specific_content['images']:
                    specific_content_str += f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                    for img_url in specific_content['images']:
                        specific_content_str += f"- {img_url}\n"
                
                # æ·»åŠ ç›¸å…³é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰
                if 'links' in specific_content and specific_content['links']:
                    specific_content_str += f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                    for link in specific_content['links'][:5]:
                        specific_content_str += f"- [{link['text']}]({link['url']})\n"
                
                # æ·»åŠ ä»£ç å—ï¼ˆå¦‚æœæœ‰ï¼Œæœ€å¤šæ˜¾ç¤º2ä¸ªï¼‰
                if 'code_blocks' in specific_content and specific_content['code_blocks']:
                    specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                    for i, code in enumerate(specific_content['code_blocks'][:2]):
                        specific_content_str += f"```\n{code}\n```\n"
                
                # æ·»åŠ å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'meta' in specific_content and specific_content['meta']:
                    meta_info = specific_content['meta']
                    specific_content_str += "\nğŸ“‹ å…ƒä¿¡æ¯:\n"
                    for key, value in meta_info.items():
                        if value:
                            specific_content_str += f"- {key}: {value}\n"
                
                # å°†ç‰¹å®šå†…å®¹æ·»åŠ åˆ°åˆ†æç»“æœä¸­
                analysis_result += specific_content_str
            
            # å¦‚æœå¯ç”¨äº†æˆªå›¾åŠŸèƒ½ï¼Œæ•è·ç½‘é¡µæˆªå›¾
            screenshot = None
            if self.enable_screenshot:
                screenshot = await analyzer.capture_screenshot(
                    url,
                    quality=self.screenshot_quality,
                    width=self.screenshot_width,
                    height=self.screenshot_height,
                    full_page=self.screenshot_full_page,
                    wait_time=self.screenshot_wait_time,
                    format=self.screenshot_format
                )
            
            # å‡†å¤‡æœ€ç»ˆçš„ç»“æœæ•°æ®
            result_data = {
                'url': url,
                'result': analysis_result,
                'screenshot': screenshot
            }
            
            # æ›´æ–°ç¼“å­˜ï¼Œä¿å­˜åˆ†æç»“æœ
            self._update_cache(url, result_data)
            
            return result_data
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œç¡®ä¿æ–¹æ³•å§‹ç»ˆè¿”å›æœ‰æ•ˆç»“æœ
            logger.error(f"å¤„ç†URL {url} æ—¶å‡ºé”™: {e}")
            return {
                'url': url,
                'result': f"âŒ å¤„ç†URLæ—¶å‡ºé”™: {url}\né”™è¯¯ä¿¡æ¯: {str(e)}",
                'screenshot': None
            }
    
    async def _batch_process_urls(self, event: AstrMessageEvent, urls: List[str]):
        """æ‰¹é‡å¤„ç†å¤šä¸ªURLï¼Œæ”¶é›†åˆ†æç»“æœå¹¶å‘é€
        
        è¯¥æ–¹æ³•è´Ÿè´£ç®¡ç†å¤šä¸ªURLçš„å¹¶å‘å¤„ç†ï¼ŒåŒ…æ‹¬ï¼š
        1. è¿‡æ»¤é‡å¤å¤„ç†çš„URL
        2. ä½¿ç”¨å¼‚æ­¥æ–¹å¼å¹¶å‘å¤„ç†å¤šä¸ªURL
        3. è°ƒç”¨_send_analysis_resultå‘é€ç»“æœ
        4. ç¡®ä¿URLå¤„ç†å®Œæˆåä»å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            urls: è¦å¤„ç†çš„URLåˆ—è¡¨
        """
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        analysis_results = []
        
        # è¿‡æ»¤æ‰æ­£åœ¨å¤„ç†çš„URLï¼Œé¿å…é‡å¤åˆ†æ
        filtered_urls = []
        for url in urls:
            if url not in self.processing_urls:
                filtered_urls.append(url)
                # æ·»åŠ åˆ°æ­£åœ¨å¤„ç†çš„é›†åˆä¸­ï¼Œé˜²æ­¢é‡å¤å¤„ç†
                self.processing_urls.add(url)
            else:
                logger.info(f"URL {url} æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤åˆ†æ")
        
        # å¦‚æœæ‰€æœ‰URLéƒ½æ­£åœ¨å¤„ç†ä¸­ï¼Œç›´æ¥è¿”å›
        if not filtered_urls:
            return
        
        try:
            # åˆ›å»ºWebAnalyzerå®ä¾‹ï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
            async with WebAnalyzer(self.max_content_length, self.timeout, self.user_agent, self.proxy, self.retry_count, self.retry_delay) as analyzer:
                # ä½¿ç”¨asyncio.gatherå¹¶å‘å¤„ç†å¤šä¸ªURLï¼Œæé«˜æ•ˆç‡
                import asyncio
                # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
                tasks = [self._process_single_url(event, url, analyzer) for url in filtered_urls]
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
                analysis_results = await asyncio.gather(*tasks)
            
            # å‘é€æ‰€æœ‰åˆ†æç»“æœ
            async for result in self._send_analysis_result(event, analysis_results):
                yield result
        finally:
            # æ— è®ºå¤„ç†æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½è¦ä»å¤„ç†é›†åˆä¸­ç§»é™¤URL
            for url in filtered_urls:
                if url in self.processing_urls:
                    self.processing_urls.remove(url)
    
    async def analyze_with_llm(self, event: AstrMessageEvent, content_data: dict) -> str:
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œå†…å®¹åˆ†æå’Œæ€»ç»“
        
        è¯¥æ–¹æ³•è´Ÿè´£è°ƒç”¨LLMå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†æï¼ŒåŒ…æ‹¬ï¼š
        1. æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
        2. è·å–åˆé€‚çš„LLMæä¾›å•†
        3. æ„å»ºä¼˜åŒ–çš„æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯ï¼‰
        4. è°ƒç”¨LLMç”Ÿæˆåˆ†æç»“æœ
        5. ç¾åŒ–å’Œæ ¼å¼åŒ–åˆ†æç»“æœ
        6. åœ¨LLMä¸å¯ç”¨æ—¶å›é€€åˆ°åŸºç¡€åˆ†æ
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µå†…å®¹,
                    'url': ç½‘é¡µURL
                }
        
        Returns:
            æ ¼å¼åŒ–çš„åˆ†æç»“æœæ–‡æœ¬
        """
        try:
            title = content_data['title']
            content = content_data['content']
            url = content_data['url']
            
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨å’Œå¯ç”¨
            if not hasattr(self.context, 'llm_generate') or not self.llm_enabled:
                # LLMä¸å¯ç”¨æˆ–æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                # æ— æ³•è·å–LLMæä¾›å•†ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)
            
            # æ„å»ºä¼˜åŒ–çš„LLMæç¤ºè¯
            emoji_prefix = "æ¯ä¸ªè¦ç‚¹ç”¨emojiå›¾æ ‡æ ‡è®°" if self.enable_emoji else ""
            
            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_prompt.format(
                    title=title,
                    url=url,
                    content=content,
                    max_length=self.max_summary_length
                )
            else:
                # é»˜è®¤æç¤ºè¯ï¼ŒåŒ…å«è¯¦ç»†çš„åˆ†æè¦æ±‚å’Œæ ¼å¼è¦æ±‚
                prompt = f"""è¯·å¯¹ä»¥ä¸‹ç½‘é¡µå†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œæ™ºèƒ½æ€»ç»“ï¼š

**ç½‘é¡µä¿¡æ¯**
- æ ‡é¢˜ï¼š{title}
- é“¾æ¥ï¼š{url}

**ç½‘é¡µå†…å®¹**ï¼š
{content}

**åˆ†æè¦æ±‚**ï¼š
1. **æ ¸å¿ƒæ‘˜è¦**ï¼šç”¨50-100å­—æ¦‚æ‹¬ç½‘é¡µçš„æ ¸å¿ƒå†…å®¹å’Œä¸»æ—¨
2. **å…³é”®è¦ç‚¹**ï¼šæå–2-3ä¸ªæœ€é‡è¦çš„ä¿¡æ¯ç‚¹æˆ–è§‚ç‚¹
3. **å†…å®¹ç±»å‹**ï¼šåˆ¤æ–­ç½‘é¡µå±äºä»€ä¹ˆç±»å‹ï¼ˆæ–°é—»ã€æ•™ç¨‹ã€åšå®¢ã€äº§å“ä»‹ç»ç­‰ï¼‰
4. **ä»·å€¼è¯„ä¼°**ï¼šç®€è¦è¯„ä»·å†…å®¹çš„ä»·å€¼å’Œå®ç”¨æ€§
5. **é€‚ç”¨äººç¾¤**ï¼šè¯´æ˜é€‚åˆå“ªäº›äººç¾¤é˜…è¯»

**è¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µç»“æ„
- {emoji_prefix}
- è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé¿å…å†—ä½™
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„æ€åº¦
- æ€»å­—æ•°ä¸è¶…è¿‡{self.max_summary_length}å­—

è¯·ç¡®ä¿åˆ†æå‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"""
            
            # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹IDè°ƒç”¨å¤§æ¨¡å‹
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,  # ä½¿ç”¨å½“å‰ä¼šè¯çš„èŠå¤©æ¨¡å‹
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                # ç¾åŒ–LLMè¿”å›çš„ç»“æœ
                analysis_text = llm_resp.completion_text.strip()
                
                # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…ç»“æœè¿‡é•¿
                if len(analysis_text) > self.max_summary_length:
                    analysis_text = analysis_text[:self.max_summary_length] + "..."
                
                # æ·»åŠ æ ‡é¢˜å’Œæ ¼å¼ç¾åŒ–
                link_emoji = "ğŸ”—" if self.enable_emoji else ""
                title_emoji = "ğŸ“" if self.enable_emoji else ""
                
                formatted_result = "**AIæ™ºèƒ½ç½‘é¡µåˆ†ææŠ¥å‘Š**\n\n"
                formatted_result += f"{link_emoji} **åˆ†æé“¾æ¥**: {url}\n"
                formatted_result += f"{title_emoji} **ç½‘é¡µæ ‡é¢˜**: {title}\n\n"
                formatted_result += "---\n\n"
                formatted_result += analysis_text
                formatted_result += "\n\n---\n"
                formatted_result += "*åˆ†æå®Œæˆï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ï¼*"
                
                return formatted_result
            else:
                # LLMè¿”å›ä¸ºç©ºï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                return self.get_enhanced_analysis(content_data)
                
        except Exception as e:
            logger.error(f"LLMåˆ†æå¤±è´¥: {e}")
            # å¦‚æœLLMåˆ†æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return f"âŒ LLMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_enhanced_analysis(self, content_data: dict) -> str:
        """å¢å¼ºç‰ˆåŸºç¡€åˆ†æï¼ˆLLMä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ¡ˆï¼‰
        
        å½“LLMä¸å¯ç”¨æˆ–å¯ç”¨æ—¶ï¼Œä½¿ç”¨æ­¤æ–¹æ³•è¿›è¡ŒåŸºç¡€åˆ†æï¼ŒåŒ…æ‹¬ï¼š
        1. å†…å®¹ç»Ÿè®¡ï¼ˆå­—ç¬¦æ•°ã€æ®µè½æ•°ã€è¯æ•°ï¼‰
        2. æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹
        3. æå–å…³é”®å¥å­ä½œä¸ºæ‘˜è¦
        4. å†…å®¹è´¨é‡è¯„ä¼°
        5. æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        
        æ”¯æŒæ ¹æ®é…ç½®æ˜¾ç¤º/éšè—emojiå’Œç»Ÿè®¡ä¿¡æ¯
        
        Args:
            content_data: åŒ…å«ç½‘é¡µå†…å®¹çš„å­—å…¸ï¼Œæ ¼å¼ä¸ºï¼š
                {
                    'title': ç½‘é¡µæ ‡é¢˜,
                    'content': ç½‘é¡µå†…å®¹,
                    'url': ç½‘é¡µURL
                }
        
        Returns:
            æ ¼å¼åŒ–çš„åŸºç¡€åˆ†æç»“æœæ–‡æœ¬
        """
        title = content_data['title']
        content = content_data['content']
        url = content_data['url']
        
        # è®¡ç®—å†…å®¹ç»Ÿè®¡ä¿¡æ¯
        char_count = len(content)
        word_count = len(content.split())
        
        # æ™ºèƒ½æ£€æµ‹å†…å®¹ç±»å‹
        content_lower = content.lower()
        content_type = "æ–‡ç« "
        if any(keyword in content_lower for keyword in ['æ–°é—»', 'æŠ¥é“', 'æ¶ˆæ¯', 'æ—¶äº‹']):
            content_type = "æ–°é—»èµ„è®¯"
        elif any(keyword in content_lower for keyword in ['æ•™ç¨‹', 'æŒ‡å—', 'æ•™å­¦', 'æ­¥éª¤', 'æ–¹æ³•']):
            content_type = "æ•™ç¨‹æŒ‡å—"
        elif any(keyword in content_lower for keyword in ['åšå®¢', 'éšç¬”', 'æ—¥è®°', 'ä¸ªäºº', 'è§‚ç‚¹']):
            content_type = "ä¸ªäººåšå®¢"
        elif any(keyword in content_lower for keyword in ['äº§å“', 'æœåŠ¡', 'è´­ä¹°', 'ä»·æ ¼', 'ä¼˜æƒ ']):
            content_type = "äº§å“ä»‹ç»"
        elif any(keyword in content_lower for keyword in ['æŠ€æœ¯', 'å¼€å‘', 'ç¼–ç¨‹', 'ä»£ç ', 'API']):
            content_type = "æŠ€æœ¯æ–‡æ¡£"
        
        # æå–å…³é”®æ®µè½ä½œä¸ºå†…å®¹æ‘˜è¦
        paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
        key_sentences = paragraphs[:3]
        
        # è¯„ä¼°å†…å®¹è´¨é‡
        quality_indicator = "å†…å®¹ä¸°å¯Œ" if char_count > 1000 else "å†…å®¹ç®€æ´"
        if char_count > 5000:
            quality_indicator = "å†…å®¹è¯¦å®"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨emoji
        robot_emoji = "ğŸ¤–" if self.enable_emoji else ""
        page_emoji = "ğŸ“„" if self.enable_emoji else ""
        info_emoji = "ğŸ“" if self.enable_emoji else ""
        stats_emoji = "ğŸ“Š" if self.enable_emoji else ""
        search_emoji = "ğŸ”" if self.enable_emoji else ""
        light_emoji = "ğŸ’¡" if self.enable_emoji else ""
        
        # æ„å»ºåˆ†æç»“æœ
        result = f"{robot_emoji} **æ™ºèƒ½ç½‘é¡µåˆ†æ** {page_emoji}\n\n"
        
        # æ·»åŠ åŸºæœ¬ä¿¡æ¯
        if self.enable_emoji:
            result += f"**{info_emoji} åŸºæœ¬ä¿¡æ¯**\n"
        else:
            result += "**åŸºæœ¬ä¿¡æ¯**\n"
        result += f"- **æ ‡é¢˜**: {title}\n"
        result += f"- **é“¾æ¥**: {url}\n"
        result += f"- **å†…å®¹ç±»å‹**: {content_type}\n"
        result += f"- **è´¨é‡è¯„ä¼°**: {quality_indicator}\n\n"
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if self.enable_statistics:
            if self.enable_emoji:
                result += f"**{stats_emoji} å†…å®¹ç»Ÿè®¡**\n"
            else:
                result += "**å†…å®¹ç»Ÿè®¡**\n"
            result += f"- å­—ç¬¦æ•°: {char_count:,}\n"
            result += f"- æ®µè½æ•°: {len(paragraphs)}\n"
            result += f"- è¯æ•°: {word_count:,}\n\n"
        
        # æ·»åŠ å†…å®¹æ‘˜è¦
        if self.enable_emoji:
            result += f"**{search_emoji} å†…å®¹æ‘˜è¦**\n"
        else:
            result += "**å†…å®¹æ‘˜è¦**\n"
        result += f"{chr(10).join(['â€¢ ' + sentence[:100] + ('...' if len(sentence) > 100 else '') for sentence in key_sentences])}\n\n"
        
        # æ·»åŠ åˆ†æè¯´æ˜
        if self.enable_emoji:
            result += f"**{light_emoji} åˆ†æè¯´æ˜**\n"
        else:
            result += "**åˆ†æè¯´æ˜**\n"
        result += "æ­¤åˆ†æåŸºäºç½‘é¡µå†…å®¹æå–ï¼Œå¦‚éœ€æ›´æ·±å…¥çš„AIæ™ºèƒ½åˆ†æï¼Œè¯·ç¡®ä¿AstrBotå·²æ­£ç¡®é…ç½®LLMåŠŸèƒ½ã€‚\n\n"
        result += "*æç¤ºï¼šå®Œæ•´å†…å®¹é¢„è§ˆè¯·æŸ¥çœ‹åŸå§‹ç½‘é¡µ*"
        
        return result
    
    @filter.command("web_config", alias={'ç½‘é¡µåˆ†æé…ç½®', 'ç½‘é¡µåˆ†æè®¾ç½®'})
    async def show_config(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºå½“å‰æ’ä»¶é…ç½®ä¿¡æ¯
        
        è¯¥å‘½ä»¤ç”¨äºæŸ¥çœ‹æ’ä»¶çš„æ‰€æœ‰é…ç½®é¡¹ï¼ŒåŒ…æ‹¬ï¼š
        - åŸºæœ¬è®¾ç½®
        - åŸŸåæ§åˆ¶
        - ç¾¤èŠæ§åˆ¶
        - åˆ†æè®¾ç½®
        - LLMé…ç½®
        - ç¿»è¯‘è®¾ç½®
        - ç¼“å­˜è®¾ç½®
        - å†…å®¹æå–è®¾ç½®
        
        ä½¿ç”¨ç¤ºä¾‹ï¼š
        /web_config
        /ç½‘é¡µåˆ†æé…ç½®
        /ç½‘é¡µåˆ†æè®¾ç½®
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        config_info = f"""**ç½‘é¡µåˆ†ææ’ä»¶é…ç½®ä¿¡æ¯**

**åŸºæœ¬è®¾ç½®**
- æœ€å¤§å†…å®¹é•¿åº¦: {self.max_content_length} å­—ç¬¦
- è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.timeout} ç§’
- LLMæ™ºèƒ½åˆ†æ: {'âœ… å·²å¯ç”¨' if self.llm_enabled else 'âŒ å·²ç¦ç”¨'}
- è‡ªåŠ¨åˆ†æé“¾æ¥: {'âœ… å·²å¯ç”¨' if self.auto_analyze else 'âŒ å·²ç¦ç”¨'}
- åˆå¹¶è½¬å‘åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.merge_forward_enabled else 'âŒ å·²ç¦ç”¨'}

**åŸŸåæ§åˆ¶**
- å…è®¸åŸŸå: {len(self.allowed_domains)} ä¸ª
- ç¦æ­¢åŸŸå: {len(self.blocked_domains)} ä¸ª

**ç¾¤èŠæ§åˆ¶**
- ç¾¤èŠé»‘åå•: {len(self.group_blacklist)} ä¸ªç¾¤èŠ

**åˆ†æè®¾ç½®**
- å¯ç”¨emoji: {'âœ… å·²å¯ç”¨' if self.enable_emoji else 'âŒ å·²ç¦ç”¨'}
- æ˜¾ç¤ºç»Ÿè®¡: {'âœ… å·²å¯ç”¨' if self.enable_statistics else 'âŒ å·²ç¦ç”¨'}
- æœ€å¤§æ‘˜è¦é•¿åº¦: {self.max_summary_length} å­—ç¬¦
- å¯ç”¨æˆªå›¾: {'âœ… å·²å¯ç”¨' if self.enable_screenshot else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾è´¨é‡: {self.screenshot_quality}
- æˆªå›¾å®½åº¦: {self.screenshot_width}px
- æˆªå›¾é«˜åº¦: {self.screenshot_height}px
- æˆªå›¾æ ¼å¼: {self.screenshot_format}
- æˆªå–æ•´é¡µ: {'âœ… å·²å¯ç”¨' if self.screenshot_full_page else 'âŒ å·²ç¦ç”¨'}
- æˆªå›¾ç­‰å¾…æ—¶é—´: {self.screenshot_wait_time}ms

**LLMé…ç½®**
- æŒ‡å®šæä¾›å•†: {self.llm_provider if self.llm_provider else 'ä½¿ç”¨ä¼šè¯é»˜è®¤'}
- è‡ªå®šä¹‰æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_prompt else 'âŒ æœªè®¾ç½®'}

**ç¿»è¯‘è®¾ç½®**
- å¯ç”¨ç½‘é¡µç¿»è¯‘: {'âœ… å·²å¯ç”¨' if self.enable_translation else 'âŒ å·²ç¦ç”¨'}
- ç›®æ ‡è¯­è¨€: {self.target_language}
- ç¿»è¯‘æä¾›å•†: {self.translation_provider}
- è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯: {'âœ… å·²å¯ç”¨' if self.custom_translation_prompt else 'âŒ æœªè®¾ç½®'}

**ç¼“å­˜è®¾ç½®**
- å¯ç”¨ç»“æœç¼“å­˜: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}
- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ
- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª

**å†…å®¹æå–è®¾ç½®**
- å¯ç”¨ç‰¹å®šå†…å®¹æå–: {'âœ… å·²å¯ç”¨' if self.enable_specific_extraction else 'âŒ å·²ç¦ç”¨'}
- æå–å†…å®¹ç±»å‹: {', '.join(self.extract_types)}

*æç¤º: å¦‚éœ€ä¿®æ”¹é…ç½®ï¼Œè¯·åœ¨AstrBotç®¡ç†é¢æ¿ä¸­ç¼–è¾‘æ’ä»¶é…ç½®*"""
        
        yield event.plain_result(config_info)
    
    @filter.command("test_merge", alias={'æµ‹è¯•åˆå¹¶è½¬å‘', 'æµ‹è¯•è½¬å‘'})
    async def test_merge_forward(self, event: AstrMessageEvent):
        """æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½
        
        è¯¥å‘½ä»¤ç”¨äºæµ‹è¯•æ’ä»¶çš„åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œä»…åœ¨ç¾¤èŠä¸­å¯ç”¨
        
        ä½¿ç”¨ç¤ºä¾‹ï¼š
        /test_merge
        /æµ‹è¯•åˆå¹¶è½¬å‘
        /æµ‹è¯•è½¬å‘
        
        åŠŸèƒ½è¯´æ˜ï¼š
        - åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘æ¶ˆæ¯
        - åŒ…å«æ ‡é¢˜èŠ‚ç‚¹å’Œä¸¤ä¸ªå†…å®¹èŠ‚ç‚¹
        - æ¼”ç¤ºåˆå¹¶è½¬å‘çš„åŸºæœ¬ç”¨æ³•
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        from astrbot.api.message_components import Node, Plain, Nodes
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ï¼Œåˆå¹¶è½¬å‘ä»…æ”¯æŒç¾¤èŠ
        group_id = None
        if hasattr(event, 'group_id') and event.group_id:
            group_id = event.group_id
        elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
            group_id = event.message_obj.group_id
        
        if group_id:
            # åˆ›å»ºæµ‹è¯•ç”¨çš„åˆå¹¶è½¬å‘èŠ‚ç‚¹
            nodes = []
            
            # æ ‡é¢˜èŠ‚ç‚¹
            title_node = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•åˆå¹¶è½¬å‘",
                content=[
                    Plain("è¿™æ˜¯åˆå¹¶è½¬å‘æµ‹è¯•æ¶ˆæ¯")
                ]
            )
            nodes.append(title_node)
            
            # å†…å®¹èŠ‚ç‚¹1
            content_node1 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹1",
                content=[
                    Plain("è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node1)
            
            # å†…å®¹èŠ‚ç‚¹2
            content_node2 = Node(
                uin=event.get_sender_id(),
                name="æµ‹è¯•èŠ‚ç‚¹2",
                content=[
                    Plain("è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•èŠ‚ç‚¹å†…å®¹")
                ]
            )
            nodes.append(content_node2)
            
            # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
            merge_forward_message = Nodes(nodes)
            yield event.chain_result([merge_forward_message])
            logger.info(f"æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½ï¼Œç¾¤èŠ {group_id}")
        else:
            yield event.plain_result("åˆå¹¶è½¬å‘åŠŸèƒ½ä»…æ”¯æŒç¾¤èŠæ¶ˆæ¯æµ‹è¯•")
            logger.info("ç§èŠæ¶ˆæ¯æ— æ³•æµ‹è¯•åˆå¹¶è½¬å‘åŠŸèƒ½")
    
    @filter.command("group_blacklist", alias={'ç¾¤é»‘åå•', 'é»‘åå•'})
    async def manage_group_blacklist(self, event: AstrMessageEvent):
        """ç®¡ç†ç¾¤èŠé»‘åå•
        
        è¯¥å‘½ä»¤ç”¨äºç®¡ç†æ’ä»¶çš„ç¾¤èŠé»‘åå•ï¼Œæ§åˆ¶å“ªäº›ç¾¤èŠä¸èƒ½ä½¿ç”¨æ’ä»¶
        
        å‘½ä»¤ç”¨æ³•ï¼š
        1. æŸ¥çœ‹é»‘åå•ï¼š/group_blacklist
        2. æ·»åŠ ç¾¤èŠï¼š/group_blacklist add <ç¾¤å·>
        3. ç§»é™¤ç¾¤èŠï¼š/group_blacklist remove <ç¾¤å·>
        4. æ¸…ç©ºé»‘åå•ï¼š/group_blacklist clear
        
        åˆ«åï¼š
        /ç¾¤é»‘åå•
        /é»‘åå•
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰é»‘åå•åˆ—è¡¨
        if len(message_parts) <= 1:
            if not self.group_blacklist:
                yield event.plain_result("å½“å‰ç¾¤èŠé»‘åå•ä¸ºç©º")
                return
            
            blacklist_info = "**å½“å‰ç¾¤èŠé»‘åå•**\n\n"
            for i, group_id in enumerate(self.group_blacklist, 1):
                blacklist_info += f"{i}. {group_id}\n"
            
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist add <ç¾¤å·>` æ·»åŠ ç¾¤èŠåˆ°é»‘åå•"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist remove <ç¾¤å·>` ä»é»‘åå•ç§»é™¤ç¾¤èŠ"
            blacklist_info += "\nä½¿ç”¨ `/group_blacklist clear` æ¸…ç©ºé»‘åå•"
            
            yield event.plain_result(blacklist_info)
            return
        
        # è§£ææ“ä½œç±»å‹å’Œå‚æ•°
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        group_id = message_parts[2] if len(message_parts) > 2 else ""
        
        # æ·»åŠ ç¾¤èŠåˆ°é»‘åå•
        if action == "add" and group_id:
            if group_id in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} å·²åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.append(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²æ·»åŠ ç¾¤èŠ {group_id} åˆ°é»‘åå•")
            
        # ä»é»‘åå•ç§»é™¤ç¾¤èŠ
        elif action == "remove" and group_id:
            if group_id not in self.group_blacklist:
                yield event.plain_result(f"ç¾¤èŠ {group_id} ä¸åœ¨é»‘åå•ä¸­")
                return
            
            self.group_blacklist.remove(group_id)
            self._save_group_blacklist()
            yield event.plain_result(f"âœ… å·²ä»é»‘åå•ç§»é™¤ç¾¤èŠ {group_id}")
            
        # æ¸…ç©ºé»‘åå•
        elif action == "clear":
            if not self.group_blacklist:
                yield event.plain_result("é»‘åå•å·²ä¸ºç©º")
                return
            
            self.group_blacklist.clear()
            self._save_group_blacklist()
            yield event.plain_result("âœ… å·²æ¸…ç©ºç¾¤èŠé»‘åå•")
            
        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: add <ç¾¤å·>, remove <ç¾¤å·>, clear")
    
    @filter.command("web_cache", alias={'ç½‘é¡µç¼“å­˜', 'æ¸…ç†ç¼“å­˜'})
    async def manage_cache(self, event: AstrMessageEvent):
        """ç®¡ç†æ’ä»¶ç¼“å­˜
        
        è¯¥å‘½ä»¤ç”¨äºç®¡ç†æ’ä»¶çš„ç½‘é¡µåˆ†æç»“æœç¼“å­˜
        
        å‘½ä»¤ç”¨æ³•ï¼š
        1. æŸ¥çœ‹ç¼“å­˜çŠ¶æ€ï¼š/web_cache
        2. æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼š/web_cache clear
        
        åˆ«åï¼š
        /ç½‘é¡µç¼“å­˜
        /æ¸…ç†ç¼“å­˜
        
        ç¼“å­˜è¯´æ˜ï¼š
        - ç¼“å­˜æœ‰æ•ˆæœŸï¼šç”±é…ç½®ä¸­çš„cache_expire_timeå†³å®š
        - æœ€å¤§ç¼“å­˜æ•°é‡ï¼šç”±é…ç½®ä¸­çš„max_cache_sizeå†³å®š
        - ç¼“å­˜åŒ…å«ï¼šç½‘é¡µåˆ†æç»“æœå’Œæˆªå›¾
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºå½“å‰ç¼“å­˜çŠ¶æ€
        if len(message_parts) <= 1:
            cache_stats = self.cache_manager.get_stats()
            cache_info = "**å½“å‰ç¼“å­˜çŠ¶æ€**\n\n"
            cache_info += f"- ç¼“å­˜æ€»æ•°: {cache_stats['total']} ä¸ª\n"
            cache_info += f"- æœ‰æ•ˆç¼“å­˜: {cache_stats['valid']} ä¸ª\n"
            cache_info += f"- è¿‡æœŸç¼“å­˜: {cache_stats['expired']} ä¸ª\n"
            cache_info += f"- ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_time} åˆ†é’Ÿ\n"
            cache_info += f"- æœ€å¤§ç¼“å­˜æ•°é‡: {self.max_cache_size} ä¸ª\n"
            cache_info += f"- ç¼“å­˜åŠŸèƒ½: {'âœ… å·²å¯ç”¨' if self.enable_cache else 'âŒ å·²ç¦ç”¨'}\n"
            
            cache_info += "\nä½¿ç”¨ `/web_cache clear` æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"
            
            yield event.plain_result(cache_info)
            return
        
        # è§£ææ“ä½œç±»å‹
        action = message_parts[1].lower() if len(message_parts) > 1 else ""
        
        # æ¸…ç©ºç¼“å­˜æ“ä½œ
        if action == "clear":
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache_manager.clear()
            cache_stats = self.cache_manager.get_stats()
            yield event.plain_result(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼Œå½“å‰ç¼“å­˜æ•°é‡: {cache_stats['total']} ä¸ª")
            
        # æ— æ•ˆæ“ä½œ
        else:
            yield event.plain_result("æ— æ•ˆçš„æ“ä½œï¼Œè¯·ä½¿ç”¨: clear")
    
    @filter.command("web_export", alias={'å¯¼å‡ºåˆ†æç»“æœ', 'ç½‘é¡µå¯¼å‡º'})
    async def export_analysis_result(self, event: AstrMessageEvent):
        """å¯¼å‡ºç½‘é¡µåˆ†æç»“æœ
        
        è¯¥å‘½ä»¤ç”¨äºå¯¼å‡ºç½‘é¡µåˆ†æç»“æœï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œå¯¼å‡ºèŒƒå›´
        
        å‘½ä»¤ç”¨æ³•ï¼š
        1. å¯¼å‡ºå•ä¸ªURLï¼š/web_export https://example.com [æ ¼å¼]
        2. å¯¼å‡ºæ‰€æœ‰ç¼“å­˜ï¼š/web_export all [æ ¼å¼]
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        - md/markdownï¼šMarkdownæ ¼å¼
        - jsonï¼šJSONæ ¼å¼
        - txtï¼šçº¯æ–‡æœ¬æ ¼å¼
        
        åˆ«åï¼š
        /å¯¼å‡ºåˆ†æç»“æœ
        /ç½‘é¡µå¯¼å‡º
        
        åŠŸèƒ½è¯´æ˜ï¼š
        - æ”¯æŒå¯¼å‡ºå•ä¸ªURLçš„åˆ†æç»“æœ
        - æ”¯æŒå¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
        - å¯¼å‡ºæ–‡ä»¶ä¼šè‡ªåŠ¨å‘é€ç»™ç”¨æˆ·
        - å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ç»“æœï¼Œä¼šå…ˆè¿›è¡Œåˆ†æ
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
        """
        # è§£æå‘½ä»¤å‚æ•°
        message_parts = event.message_str.strip().split()
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦è¶³å¤Ÿ
        if len(message_parts) < 2:
            yield event.plain_result("è¯·æä¾›è¦å¯¼å‡ºçš„URLé“¾æ¥å’Œæ ¼å¼ï¼Œä¾‹å¦‚ï¼š/web_export https://example.com md æˆ– /web_export all json")
            return
        
        # è·å–å¯¼å‡ºèŒƒå›´å’Œæ ¼å¼
        url_or_all = message_parts[1]
        format_type = message_parts[2] if len(message_parts) > 2 else "md"
        
        # éªŒè¯æ ¼å¼ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_formats = ["md", "markdown", "json", "txt"]
        if format_type.lower() not in supported_formats:
            yield event.plain_result(f"ä¸æ”¯æŒçš„æ ¼å¼ç±»å‹ï¼Œè¯·ä½¿ç”¨ï¼š{', '.join(supported_formats)}")
            return
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_results = []
        
        if url_or_all.lower() == "all":
            # å¯¼å‡ºæ‰€æœ‰ç¼“å­˜çš„åˆ†æç»“æœ
            if not self.cache:
                yield event.plain_result("å½“å‰æ²¡æœ‰ç¼“å­˜çš„åˆ†æç»“æœ")
                return
            
            for url, cache_data in self.cache.items():
                export_results.append({
                    "url": url,
                    "result": cache_data["result"]
                })
        else:
            # å¯¼å‡ºæŒ‡å®šURLçš„åˆ†æç»“æœ
            url = url_or_all
            
            # æ£€æŸ¥URLæ ¼å¼æ˜¯å¦æœ‰æ•ˆ
            if not self.analyzer.is_valid_url(url):
                yield event.plain_result("æ— æ•ˆçš„URLé“¾æ¥")
                return
            
            # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰è¯¥URLçš„åˆ†æç»“æœ
            cached_result = self._check_cache(url)
            if cached_result:
                export_results.append({
                    "url": url,
                    "result": cached_result
                })
            else:
                # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆè¿›è¡Œåˆ†æ
                yield event.plain_result("ç¼“å­˜ä¸­æ²¡æœ‰è¯¥URLçš„åˆ†æç»“æœï¼Œæ­£åœ¨è¿›è¡Œåˆ†æ...")
                
                # æŠ“å–å¹¶åˆ†æç½‘é¡µ
                async with WebAnalyzer(self.max_content_length, self.timeout, self.user_agent, self.proxy, self.retry_count, self.retry_delay) as analyzer:
                    html = await analyzer.fetch_webpage(url)
                    if not html:
                        yield event.plain_result(f"æ— æ³•æŠ“å–ç½‘é¡µå†…å®¹: {url}")
                        return
                    
                    content_data = analyzer.extract_content(html, url)
                    if not content_data:
                        yield event.plain_result(f"æ— æ³•è§£æç½‘é¡µå†…å®¹: {url}")
                        return
                    
                    # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
                    if self.enable_translation:
                        translated_content = await self._translate_content(event, content_data['content'])
                        translated_content_data = content_data.copy()
                        translated_content_data['content'] = translated_content
                        analysis_result = await self.analyze_with_llm(event, translated_content_data)
                    else:
                        analysis_result = await self.analyze_with_llm(event, content_data)
                    
                    # æå–ç‰¹å®šå†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    specific_content = self._extract_specific_content(html, url)
                    if specific_content:
                        # åœ¨åˆ†æç»“æœä¸­æ·»åŠ ç‰¹å®šå†…å®¹
                        specific_content_str = "\n\n**ç‰¹å®šå†…å®¹æå–**\n"
                        
                        if 'images' in specific_content and specific_content['images']:
                            specific_content_str += f"\nğŸ“· å›¾ç‰‡é“¾æ¥ ({len(specific_content['images'])}):\n"
                            for img_url in specific_content['images']:
                                specific_content_str += f"- {img_url}\n"
                        
                        if 'links' in specific_content and specific_content['links']:
                            specific_content_str += f"\nğŸ”— ç›¸å…³é“¾æ¥ ({len(specific_content['links'])}):\n"
                            for link in specific_content['links'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé“¾æ¥
                                specific_content_str += f"- [{link['text']}]({link['url']})\n"
                        
                        if 'code_blocks' in specific_content and specific_content['code_blocks']:
                            specific_content_str += f"\nğŸ’» ä»£ç å— ({len(specific_content['code_blocks'])}):\n"
                            for i, code in enumerate(specific_content['code_blocks'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£ç å—
                                specific_content_str += f"```\n{code}\n```\n"
                        
                        analysis_result += specific_content_str
                    
                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_results.append({
                        "url": url,
                        "result": {
                            "url": url,
                            "result": analysis_result,
                            "screenshot": None
                        }
                    })
        
        # æ‰§è¡Œå¯¼å‡ºæ“ä½œ
        try:
            import os
            import json
            import time
            
            # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            data_dir = os.path.join(os.path.dirname(__file__), "data")
            os.makedirs(data_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = int(time.time())
            if len(export_results) == 1:
                # å•ä¸ªURLå¯¼å‡ºï¼Œä½¿ç”¨åŸŸåä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                url = export_results[0]["url"]
                from urllib.parse import urlparse
                parsed = urlparse(url)
                domain = parsed.netloc.replace(".", "_")
                filename = f"web_analysis_{domain}_{timestamp}"
            else:
                # å¤šä¸ªURLå¯¼å‡º
                filename = f"web_analysis_all_{timestamp}"
            
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            file_extension = format_type.lower()
            if file_extension == "markdown":
                file_extension = "md"
            
            file_path = os.path.join(data_dir, f"{filename}.{file_extension}")
            
            if format_type.lower() in ["md", "markdown"]:
                # ç”ŸæˆMarkdownæ ¼å¼å†…å®¹
                md_content = "# ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n\n"
                md_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n\n"
                md_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n\n"
                md_content += "---\n\n"
                
                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    md_content += f"## {i}. {url}\n\n"
                    md_content += result_data["result"]
                    md_content += "\n\n"
                    md_content += "---\n\n"
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(md_content)
            
            elif format_type.lower() == "json":
                # ç”ŸæˆJSONæ ¼å¼å†…å®¹
                json_data = {
                    "export_time": timestamp,
                    "export_time_str": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp)),
                    "total_results": len(export_results),
                    "results": []
                }
                
                for export_item in export_results:
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    json_data["results"].append({
                        "url": url,
                        "analysis_result": result_data["result"],
                        "has_screenshot": result_data["screenshot"] is not None
                    })
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            elif format_type.lower() == "txt":
                # ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼å†…å®¹
                txt_content = "ç½‘é¡µåˆ†æç»“æœå¯¼å‡º\n"
                txt_content += f"å¯¼å‡ºæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))}\n"
                txt_content += f"å…± {len(export_results)} ä¸ªåˆ†æç»“æœ\n"
                txt_content += "=" * 50 + "\n\n"
                
                for i, export_item in enumerate(export_results, 1):
                    url = export_item["url"]
                    result_data = export_item["result"]
                    
                    txt_content += f"{i}. {url}\n"
                    txt_content += "-" * 30 + "\n"
                    txt_content += result_data["result"]
                    txt_content += "\n\n"
                    txt_content += "=" * 50 + "\n\n"
                
                # å†™å…¥æ–‡ä»¶
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(txt_content)
            
            # å‘é€å¯¼å‡ºæˆåŠŸæ¶ˆæ¯ï¼Œå¹¶é™„å¸¦å¯¼å‡ºæ–‡ä»¶
            from astrbot.api.message_components import Plain, File
            
            # æ„å»ºæ¶ˆæ¯é“¾
            message_chain = [
                    Plain("âœ… åˆ†æç»“æœå¯¼å‡ºæˆåŠŸï¼\n\n"),
                    Plain(f"å¯¼å‡ºæ ¼å¼: {format_type}\n"),
                    Plain(f"å¯¼å‡ºæ•°é‡: {len(export_results)}\n\n"),
                    Plain("ğŸ“ å¯¼å‡ºæ–‡ä»¶ï¼š\n"),
                    File(file=file_path, name=os.path.basename(file_path))
                ]
            
            yield event.chain_result(message_chain)
            
            logger.info(f"æˆåŠŸå¯¼å‡º {len(export_results)} ä¸ªåˆ†æç»“æœåˆ° {file_path}ï¼Œå¹¶å‘é€ç»™ç”¨æˆ·")
        
        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å¯¼å‡ºåˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    def _save_group_blacklist(self):
        """ä¿å­˜ç¾¤èŠé»‘åå•åˆ°é…ç½®æ–‡ä»¶
        
        è¯¥æ–¹æ³•è´Ÿè´£å°†ç¾¤èŠé»‘åå•åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œå¹¶ä¿å­˜åˆ°é…ç½®ä¸­
        
        åŠŸèƒ½è¯´æ˜ï¼š
        - å°†ç¾¤èŠIDåˆ—è¡¨è½¬æ¢ä¸ºæ¢è¡Œåˆ†éš”çš„æ–‡æœ¬
        - æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„group_blacklistå­—æ®µ
        - ä¿å­˜é…ç½®æ›´æ”¹
        - å¤„ç†ä¿å­˜è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¼‚å¸¸
        """
        try:
            # å°†ç¾¤èŠåˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç¾¤èŠID
            group_text = '\n'.join(self.group_blacklist)
            # æ›´æ–°é…ç½®å¹¶ä¿å­˜åˆ°æ–‡ä»¶
            self.config['group_blacklist'] = group_text
            self.config.save_config()
        except Exception as e:
            logger.error(f"ä¿å­˜ç¾¤èŠé»‘åå•å¤±è´¥: {e}")
    
    def _check_cache(self, url: str) -> dict:
        """æ£€æŸ¥æŒ‡å®šURLçš„ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        
        Args:
            url: è¦æ£€æŸ¥ç¼“å­˜çš„ç½‘é¡µURL
            
        Returns:
            - å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè¿”å›ç¼“å­˜çš„åˆ†æç»“æœ
            - å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–æ— æ•ˆï¼Œè¿”å›None
        """
        if not self.enable_cache:
            return None
        
        return self.cache_manager.get(url)
    
    def _update_cache(self, url: str, result: dict):
        """æ›´æ–°æŒ‡å®šURLçš„ç¼“å­˜
        
        Args:
            url: è¦æ›´æ–°ç¼“å­˜çš„ç½‘é¡µURL
            result: åŒ…å«åˆ†æç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸_check_cacheè¿”å›å€¼ä¸€è‡´
        """
        if not self.enable_cache:
            return
        
        self.cache_manager.set(url, result)
    
    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜
        
        æ³¨æ„ï¼šç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œå› æ­¤è¯¥æ–¹æ³•ç›®å‰ç•™ç©º
        å¦‚éœ€æ‰‹åŠ¨æ¸…ç†ï¼Œå¯ä»¥åœ¨æ­¤æ–¹æ³•ä¸­æ·»åŠ ç›¸åº”é€»è¾‘
        """
        # ç¼“å­˜ç®¡ç†å™¨ä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œè¿™é‡Œç•™ç©ºå³å¯
        pass
    
    async def _translate_content(self, event: AstrMessageEvent, content: str) -> str:
        """ç¿»è¯‘ç½‘é¡µå†…å®¹
        
        è¯¥æ–¹æ³•è´Ÿè´£è°ƒç”¨LLMå¯¹ç½‘é¡µå†…å®¹è¿›è¡Œç¿»è¯‘
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            content: è¦ç¿»è¯‘çš„ç½‘é¡µå†…å®¹
            
        Returns:
            - å¦‚æœç¿»è¯‘æˆåŠŸï¼Œè¿”å›ç¿»è¯‘åçš„å†…å®¹
            - å¦‚æœç¿»è¯‘å¤±è´¥æˆ–æœªå¯ç”¨ç¿»è¯‘ï¼Œè¿”å›åŸå§‹å†…å®¹
        """
        if not self.enable_translation:
            return content
        
        try:
            # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
            if not hasattr(self.context, 'llm_generate'):
                logger.error("LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„LLMæä¾›å•†ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨å½“å‰ä¼šè¯çš„æ¨¡å‹
            provider_id = self.llm_provider
            if not provider_id:
                umo = event.unified_msg_origin
                provider_id = await self.context.get_current_chat_provider_id(umo=umo)
            
            if not provider_id:
                logger.error("æ— æ³•è·å–LLMæä¾›å•†IDï¼Œæ— æ³•è¿›è¡Œç¿»è¯‘")
                return content
            
            # ä½¿ç”¨è‡ªå®šä¹‰ç¿»è¯‘æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if self.custom_translation_prompt:
                # æ›¿æ¢è‡ªå®šä¹‰æç¤ºè¯ä¸­çš„å˜é‡
                prompt = self.custom_translation_prompt.format(
                    content=content,
                    target_language=self.target_language
                )
            else:
                # é»˜è®¤ç¿»è¯‘æç¤ºè¯
                prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{self.target_language}è¯­è¨€ï¼Œä¿æŒåŸæ–‡æ„æ€ä¸å˜ï¼Œè¯­è¨€æµç•…è‡ªç„¶ï¼š\n\n{content}"
            
            # è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘
            llm_resp = await self.context.llm_generate(
                chat_provider_id=provider_id,
                prompt=prompt
            )
            
            if llm_resp and llm_resp.completion_text:
                return llm_resp.completion_text.strip()
            else:
                logger.error("LLMç¿»è¯‘è¿”å›ä¸ºç©º")
                return content
        except Exception as e:
            logger.error(f"ç¿»è¯‘å†…å®¹å¤±è´¥: {e}")
            return content
    
    def _extract_specific_content(self, html: str, url: str) -> dict:
        """æå–ç‰¹å®šç±»å‹çš„å†…å®¹
        
        è¯¥æ–¹æ³•è´Ÿè´£ä»HTMLä¸­æå–ç‰¹å®šç±»å‹çš„å†…å®¹ï¼Œå¦‚å›¾ç‰‡ã€é“¾æ¥ã€ä»£ç å—ç­‰
        
        Args:
            html: ç½‘é¡µHTMLå†…å®¹
            url: ç½‘é¡µURL
            
        Returns:
            - å¦‚æœæå–æˆåŠŸï¼Œè¿”å›åŒ…å«ç‰¹å®šå†…å®¹çš„å­—å…¸
            - å¦‚æœæå–å¤±è´¥æˆ–æœªå¯ç”¨ç‰¹å®šå†…å®¹æå–ï¼Œè¿”å›ç©ºå­—å…¸
        """
        if not self.enable_specific_extraction:
            return {}
        
        try:
            # ä½¿ç”¨WebAnalyzerå®ä¾‹æå–ç‰¹å®šå†…å®¹
            analyzer = WebAnalyzer(self.max_content_length, self.timeout, self.user_agent)
            return analyzer.extract_specific_content(html, url, self.extract_types)
        except Exception as e:
            logger.error(f"æå–ç‰¹å®šå†…å®¹å¤±è´¥: {e}")
            return {}
    
    async def _send_analysis_result(self, event, analysis_results):
        """å‘é€åˆ†æç»“æœï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨åˆå¹¶è½¬å‘
        
        è¯¥æ–¹æ³•è´Ÿè´£å°†åˆ†æç»“æœå‘é€ç»™ç”¨æˆ·ï¼Œæ”¯æŒæ™®é€šæ¶ˆæ¯å’Œåˆå¹¶è½¬å‘ä¸¤ç§æ–¹å¼
        
        Args:
            event: æ¶ˆæ¯äº‹ä»¶å¯¹è±¡
            analysis_results: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„åˆ—è¡¨
        """
        try:
            from astrbot.api.message_components import Node, Plain, Nodes, Image
            import tempfile
            import os
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨
            group_id = None
            if hasattr(event, 'group_id') and event.group_id:
                group_id = event.group_id
            elif hasattr(event, 'message_obj') and hasattr(event.message_obj, 'group_id') and event.message_obj.group_id:
                group_id = event.message_obj.group_id
            
            # å¦‚æœæ˜¯ç¾¤èŠæ¶ˆæ¯ä¸”åˆå¹¶è½¬å‘åŠŸèƒ½å·²å¯ç”¨ï¼Œä½¿ç”¨åˆå¹¶è½¬å‘
            if group_id and self.merge_forward_enabled:
                # ä½¿ç”¨åˆå¹¶è½¬å‘ - å°†æ‰€æœ‰åˆ†æç»“æœåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                nodes = []
                
                # æ·»åŠ æ€»æ ‡é¢˜èŠ‚ç‚¹
                total_title_node = Node(
                    uin=event.get_sender_id(),
                    name="ç½‘é¡µåˆ†æç»“æœæ±‡æ€»",
                    content=[
                        Plain(f"å…±{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœ")
                    ]
                )
                nodes.append(total_title_node)
                
                # ä¸ºæ¯ä¸ªURLæ·»åŠ åˆ†æç»“æœèŠ‚ç‚¹
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data['url']
                    analysis_result = result_data['result']
                    screenshot = result_data.get('screenshot')
                    
                    # æ·»åŠ å½“å‰URLçš„æ ‡é¢˜èŠ‚ç‚¹
                    url_title_node = Node(
                        uin=event.get_sender_id(),
                        name=f"åˆ†æç»“æœ {i}",
                        content=[
                            Plain(f"ç¬¬{i}ä¸ªç½‘é¡µåˆ†æç»“æœ - {url}")
                        ]
                    )
                    nodes.append(url_title_node)
                    
                    # æ·»åŠ å½“å‰URLçš„å†…å®¹èŠ‚ç‚¹
                    content_node = Node(
                        uin=event.get_sender_id(),
                        name="è¯¦ç»†åˆ†æ",
                        content=[
                            Plain(analysis_result)
                        ]
                    )
                    nodes.append(content_node)
            
                # ä½¿ç”¨NodesåŒ…è£…æ‰€æœ‰èŠ‚ç‚¹ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆå¹¶è½¬å‘æ¶ˆæ¯
                merge_forward_message = Nodes(nodes)
                
                # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
                yield event.chain_result([merge_forward_message])
                
                # å¦‚æœæœ‰æˆªå›¾ï¼Œé€ä¸ªå‘é€æˆªå›¾
                for result_data in analysis_results:
                    screenshot = result_data.get('screenshot')
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = f".{self.screenshot_format}" if self.screenshot_format else ".jpg"
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name
                            
                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                logger.info(f"ç¾¤èŠ {group_id} ä½¿ç”¨åˆå¹¶è½¬å‘å‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
            else:
                # æ™®é€šå‘é€ - é€ä¸ªå‘é€åˆ†æç»“æœ
                for i, result_data in enumerate(analysis_results, 1):
                    url = result_data['url']
                    analysis_result = result_data['result']
                    screenshot = result_data.get('screenshot')
                    
                    # å‘é€åˆ†æç»“æœæ–‡æœ¬
                    if len(analysis_results) == 1:
                        result_text = f"ç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    else:
                        result_text = f"ç¬¬{i}/{len(analysis_results)}ä¸ªç½‘é¡µåˆ†æç»“æœï¼š\n{analysis_result}"
                    yield event.plain_result(result_text)
                    
                    # å¦‚æœæœ‰æˆªå›¾ï¼Œå‘é€æˆªå›¾
                    if screenshot:
                        try:
                            # æ ¹æ®æˆªå›¾æ ¼å¼è®¾ç½®æ–‡ä»¶åç¼€
                            suffix = f".{self.screenshot_format}" if self.screenshot_format else ".jpg"
                            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆªå›¾
                            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as temp_file:
                                temp_file.write(screenshot)
                                temp_file_path = temp_file.name
                            
                            # ä½¿ç”¨Image.fromFileSystem()æ–¹æ³•å‘é€å›¾ç‰‡
                            image_component = Image.fromFileSystem(temp_file_path)
                            yield event.chain_result([image_component])
                            logger.info("æ™®é€šå‘é€åˆ†æç»“æœï¼Œå¹¶å‘é€æˆªå›¾")
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.unlink(temp_file_path)
                        except Exception as e:
                            logger.error(f"å‘é€æˆªå›¾å¤±è´¥: {e}")
                            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«åˆ é™¤
                            if 'temp_file_path' in locals() and os.path.exists(temp_file_path):
                                os.unlink(temp_file_path)
                message_type = "ç¾¤èŠ" if group_id else "ç§èŠ"
                logger.info(f"{message_type}æ¶ˆæ¯æ™®é€šå‘é€{len(analysis_results)}ä¸ªåˆ†æç»“æœ")
        except Exception as e:
            logger.error(f"å‘é€åˆ†æç»“æœå¤±è´¥: {e}")
            yield event.plain_result(f"âŒ å‘é€åˆ†æç»“æœå¤±è´¥: {str(e)}")
    
    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ç½‘é¡µåˆ†ææ’ä»¶å·²å¸è½½")